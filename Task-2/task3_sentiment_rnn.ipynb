{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2.3: Text classification via RNN (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will perform sentiment analysis of the IMDBs reviews by using RNN. An additional goal is to learn high abstactions of the **torchtext** module that consists of data processing utilities and popular datasets for natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torchtext import datasets\n",
    "\n",
    "from torchtext.data import Field, LabelField\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True, lower=True, batch_first=True)\n",
    "LABEL = LabelField(batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train, tst = datasets.IMDB.splits(TEXT, LABEL)\n",
    "trn, vld = train.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 913 ms, sys: 22.8 ms, total: 936 ms\n",
      "Wall time: 937 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TEXT.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocab.freqs is a collections.Counter object, so we can take a look at the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 223801),\n",
       " ('a', 111451),\n",
       " ('and', 110409),\n",
       " ('of', 100200),\n",
       " ('to', 93193),\n",
       " ('is', 72635),\n",
       " ('in', 62957),\n",
       " ('i', 49539),\n",
       " ('this', 48863),\n",
       " ('that', 46089)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Iterator (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we'll be using a special kind of Iterator, called the **BucketIterator**. When we pass data into a neural network, we want the data to be padded to be the same length so that we can process them in batch:\n",
    "\n",
    "e.g.\n",
    "\\[ \n",
    "\\[3, 15, 2, 7\\],\n",
    "\\[4, 1\\], \n",
    "\\[5, 5, 6, 8, 1\\] \n",
    "\\] -> \\[ \n",
    "\\[3, 15, 2, 7, **0**\\],\n",
    "\\[4, 1, **0**, **0**, **0**\\], \n",
    "\\[5, 5, 6, 8, 1\\] \n",
    "\\] \n",
    "\n",
    "If the sequences differ greatly in length, the padding will consume a lot of wasteful memory and time. The BucketIterator groups sequences of similar lengths together for each batch to minimize padding.\n",
    "\n",
    "Complete the definition of the **BucketIterator** object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    "        (trn, vld, tst),\n",
    "        batch_sizes=(64, 64, 64),\n",
    "        sort=True,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=False,\n",
    "        device='cuda',\n",
    "        repeat=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what the output of the BucketIterator looks like. Do not be suprised **batch_first=True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    9,   518,   845,  ...,     1,     1,     1],\n",
       "        [   10,    20,     7,  ...,     1,     1,     1],\n",
       "        [ 1331,   136,  2280,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [   10,    25,     7,  ...,   300,    17,  3521],\n",
       "        [ 1850,  2434,    16,  ...,   115,    16,  2166],\n",
       "        [   10,    20,     7,  ..., 13938,    13,   695]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(train_iter.__iter__()); batch.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 34])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch has all the fields we passed to the Dataset as attributes. The batch data can be accessed through the attribute with the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['batch_size', 'dataset', 'fields', 'input_fields', 'target_fields', 'text', 'label'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the RNN-based text classification model (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start simple first. Implement the model according to the shema below.  \n",
    "![alt text](https://miro.medium.com/max/1396/1*v-tLYQCsni550A-hznS0mw.jpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNBaseline(nn.Module):\n",
    "    def __init__(self, hidden_dim, emb_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "            \n",
    "    def forward(self, seq):\n",
    "        embedded = self.emb(seq)\n",
    "        h_seq, h_n = self.gru(embedded)\n",
    "        logits = self.fc(h_n).view(x.shape[0], -1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNBaseline(\n",
       "  (emb): Embedding(201057, 200)\n",
       "  (gru): GRU(200, 300, batch_first=True)\n",
       "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_sz = 200\n",
    "nh = 300\n",
    "model = RNNBaseline(nh, emb_dim=em_sz); model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using a GPU, remember to call model.cuda() to move your model to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNBaseline(\n",
       "  (emb): Embedding(201057, 200)\n",
       "  (gru): GRU(200, 300, batch_first=True)\n",
       "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training loop (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimization and the loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=3e-4)\n",
    "loss_func = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the stopping criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.010202687626225608, Validation Loss: 0.009948110779126485\n",
      "Epoch: 2, Training Loss: 0.008900403203283037, Validation Loss: 0.010478779538472493\n",
      "Epoch: 3, Training Loss: 0.008424819031783513, Validation Loss: 0.008872889057795206\n",
      "Epoch: 4, Training Loss: 0.005823815512657165, Validation Loss: 0.006868855317433675\n",
      "Epoch: 5, Training Loss: 0.00452646757704871, Validation Loss: 0.0074263299147288\n",
      "CPU times: user 47.9 s, sys: 503 ms, total: 48.4 s\n",
      "Wall time: 48.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() \n",
    "    for batch in train_iter: \n",
    "        \n",
    "        x = batch.text\n",
    "        y = batch.label.view(-1, 1).type(torch.float)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        preds = model(x)   \n",
    "        loss = loss_func(preds, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(trn)\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    for batch in val_iter:\n",
    "        \n",
    "        x = batch.text\n",
    "        y = batch.label.view(-1, 1).type(torch.float)\n",
    "        \n",
    "        preds = model(x) \n",
    "        loss = loss_func(preds, y)\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "    val_loss /= len(vld)\n",
    "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate performance of the trained model (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.786\n",
      "precision: 0.766\n",
      "recall: 0.824\n",
      "f1: 0.794\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_true = np.zeros(len(tst))\n",
    "y_pred = np.zeros(len(tst))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_iter):\n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "        y_batch_pred = torch.exp(model(x))\n",
    "        y_true[i * 64 : (i + 1) * 64] = y.cpu().numpy()\n",
    "        y_pred[i * 64 : (i + 1) * 64] = y_batch_pred.cpu().numpy().flatten() > 0.5\n",
    "\n",
    "print(f'accuracy: {round(accuracy_score(y_true, y_pred), 3)}')\n",
    "print(f'precision: {round(precision_score(y_true, y_pred), 3)}')\n",
    "print(f'recall: {round(recall_score(y_true, y_pred), 3)}')\n",
    "print(f'f1: {round(f1_score(y_true, y_pred), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down the calculated performance\n",
    "\n",
    "### Accuracy: 0.786\n",
    "### Precision: 0.766\n",
    "### Recall: 0.824\n",
    "### F1: 0.794"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments (10 points)\n",
    "\n",
    "Experiment with the model and achieve better results. You can find advices [here](https://arxiv.org/abs/1801.06146). Implement and describe your experiments in details, mention what was helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Replace GRU with LSTM (minor f1-score improvement)\n",
    "### 2. Stack more GRU layers (no improvement)\n",
    "### 3. Stack more linear layers(minor f1-score improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Replace GRU with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_RNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, emb_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, len(LABEL.vocab))\n",
    "            \n",
    "    def forward(self, seq):\n",
    "        embedded = self.emb(seq)\n",
    "        h_seq, h_n = self.lstm(embedded)\n",
    "        logits = self.fc(h_n).view(x.shape[0], -1)\n",
    "        return logits\n",
    "\n",
    "epochs = 5\n",
    "em_sz = 200\n",
    "nh = 300\n",
    "model = RNNBaseline(nh, emb_dim=em_sz)\n",
    "model.cuda()\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=3e-4)\n",
    "loss_func = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.010394512609073094, Validation Loss: 0.010490611632664999\n",
      "Epoch: 2, Training Loss: 0.008800734301975794, Validation Loss: 0.009240469149748484\n",
      "Epoch: 3, Training Loss: 0.00720909207378115, Validation Loss: 0.010440164800484975\n",
      "Epoch: 4, Training Loss: 0.006119077843427658, Validation Loss: 0.007283803216616313\n",
      "Epoch: 5, Training Loss: 0.00468870895419802, Validation Loss: 0.0075952236930529275\n",
      "CPU times: user 48.9 s, sys: 239 ms, total: 49.1 s\n",
      "Wall time: 49.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() \n",
    "    for batch in train_iter: \n",
    "        \n",
    "        x = batch.text\n",
    "        y = batch.label.view(-1, 1).type(torch.float)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        preds = model(x)   \n",
    "        loss = loss_func(preds, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(trn)\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    for batch in val_iter:\n",
    "        \n",
    "        x = batch.text\n",
    "        y = batch.label.view(-1, 1).type(torch.float)\n",
    "        \n",
    "        preds = model(x) \n",
    "        loss = loss_func(preds, y)\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "    val_loss /= len(vld)\n",
    "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.799\n",
      "precision: 0.801\n",
      "recall: 0.797\n",
      "f1: 0.799\n"
     ]
    }
   ],
   "source": [
    "y_true = np.zeros(len(tst))\n",
    "y_pred = np.zeros(len(tst))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_iter):\n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "        y_batch_pred = torch.exp(model(x))\n",
    "        y_true[i * 64 : (i + 1) * 64] = y.cpu().numpy()\n",
    "        y_pred[i * 64 : (i + 1) * 64] = y_batch_pred.cpu().numpy().flatten() > 0.5\n",
    "\n",
    "print(f'accuracy: {round(accuracy_score(y_true, y_pred), 3)}')\n",
    "print(f'precision: {round(precision_score(y_true, y_pred), 3)}')\n",
    "print(f'recall: {round(recall_score(y_true, y_pred), 3)}')\n",
    "print(f'f1: {round(f1_score(y_true, y_pred), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stack more GRU layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.010115732039724077, Validation Loss: 0.010826970005035401\n",
      "Epoch: 2, Training Loss: 0.009827562093734742, Validation Loss: 0.009730577715237936\n",
      "Epoch: 3, Training Loss: 0.00772027542420796, Validation Loss: 0.008830604835351308\n",
      "Epoch: 4, Training Loss: 0.0062943147114345, Validation Loss: 0.008755472727616629\n",
      "Epoch: 5, Training Loss: 0.005029705234936305, Validation Loss: 0.00799410094022751\n",
      "accuracy: 0.733\n",
      "precision: 0.663\n",
      "recall: 0.944\n",
      "f1: 0.779\n"
     ]
    }
   ],
   "source": [
    "class RNN_GRU_MORE_LAYERS(nn.Module):\n",
    "    def __init__(self, hidden_dim, emb_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, num_layers=3, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, len(LABEL.vocab))\n",
    "            \n",
    "    def forward(self, seq):\n",
    "        embedded = self.emb(seq)\n",
    "        h_seq, h_n = self.gru(embedded)\n",
    "        logits = self.fc(h_n).view(x.shape[0], -1)\n",
    "        return logits\n",
    "\n",
    "epochs = 5\n",
    "em_sz = 200\n",
    "nh = 300\n",
    "model = RNNBaseline(nh, emb_dim=em_sz)\n",
    "model.cuda()\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=3e-4)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() \n",
    "    for batch in train_iter: \n",
    "        \n",
    "        x = batch.text\n",
    "        y = batch.label.view(-1, 1).type(torch.float)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        preds = model(x)   \n",
    "        loss = loss_func(preds, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(trn)\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    for batch in val_iter:\n",
    "        \n",
    "        x = batch.text\n",
    "        y = batch.label.view(-1, 1).type(torch.float)\n",
    "        \n",
    "        preds = model(x) \n",
    "        loss = loss_func(preds, y)\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "    val_loss /= len(vld)\n",
    "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, epoch_loss, val_loss))\n",
    "\n",
    "y_true = np.zeros(len(tst))\n",
    "y_pred = np.zeros(len(tst))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_iter):\n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "        y_batch_pred = torch.exp(model(x))\n",
    "        y_true[i * 64 : (i + 1) * 64] = y.cpu().numpy()\n",
    "        y_pred[i * 64 : (i + 1) * 64] = y_batch_pred.cpu().numpy().flatten() > 0.5\n",
    "\n",
    "print(f'accuracy: {round(accuracy_score(y_true, y_pred), 3)}')\n",
    "print(f'precision: {round(precision_score(y_true, y_pred), 3)}')\n",
    "print(f'recall: {round(recall_score(y_true, y_pred), 3)}')\n",
    "print(f'f1: {round(f1_score(y_true, y_pred), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stack more linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.010186308268138341, Validation Loss: 0.010147208086649576\n",
      "Epoch: 2, Training Loss: 0.008149138351849147, Validation Loss: 0.010538814282417297\n",
      "Epoch: 3, Training Loss: 0.009862441568715232, Validation Loss: 0.009839516989390056\n",
      "Epoch: 4, Training Loss: 0.007747751390933991, Validation Loss: 0.007571780848503113\n",
      "Epoch: 5, Training Loss: 0.005280466116326196, Validation Loss: 0.007482968397935231\n",
      "accuracy: 0.771\n",
      "precision: 0.714\n",
      "recall: 0.904\n",
      "f1: 0.798\n"
     ]
    }
   ],
   "source": [
    "class RNN_MORE_LAYERS_FOR_THE_GOD_OF_LAYERS(nn.Module):\n",
    "    def __init__(self, hidden_dim, emb_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "            \n",
    "    def forward(self, seq):\n",
    "        embedded = self.emb(seq)\n",
    "        h_seq, h_n = self.gru(embedded)\n",
    "        fc_out = self.relu(self.fc1(h_n))\n",
    "        logits = self.fc2(fc_out).view(x.shape[0], -1)\n",
    "        return logits\n",
    "\n",
    "epochs = 5\n",
    "em_sz = 200\n",
    "nh = 300\n",
    "model = RNN_MORE_LAYERS_FOR_THE_GOD_OF_LAYERS(nh, emb_dim=em_sz)\n",
    "model.cuda()\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=3e-4)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() \n",
    "    for batch in train_iter: \n",
    "        \n",
    "        x = batch.text\n",
    "        y = batch.label.view(-1, 1).type(torch.float)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        preds = model(x)   \n",
    "        loss = loss_func(preds, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(trn)\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    for batch in val_iter:\n",
    "        \n",
    "        x = batch.text\n",
    "        y = batch.label.view(-1, 1).type(torch.float)\n",
    "        \n",
    "        preds = model(x) \n",
    "        loss = loss_func(preds, y)\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "    val_loss /= len(vld)\n",
    "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, epoch_loss, val_loss))\n",
    "\n",
    "y_true = np.zeros(len(tst))\n",
    "y_pred = np.zeros(len(tst))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_iter):\n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "        y_batch_pred = torch.exp(model(x))\n",
    "        y_true[i * 64 : (i + 1) * 64] = y.cpu().numpy()\n",
    "        y_pred[i * 64 : (i + 1) * 64] = y_batch_pred.cpu().numpy().flatten() > 0.5\n",
    "\n",
    "print(f'accuracy: {round(accuracy_score(y_true, y_pred), 3)}')\n",
    "print(f'precision: {round(precision_score(y_true, y_pred), 3)}')\n",
    "print(f'recall: {round(recall_score(y_true, y_pred), 3)}')\n",
    "print(f'f1: {round(f1_score(y_true, y_pred), 3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
